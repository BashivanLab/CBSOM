{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import euclidean\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from spacetorch.paths import *\n",
    "from spacetorch.datasets import floc\n",
    "from spacetorch.datasets import DatasetRegistry\n",
    "from spacetorch.models.trunks.resnet_chanx import SOBasicBlock,SOResNet,SOCONV\n",
    "from spacetorch.analyses.core import select_model\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "device_id = 4 \n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchvision.models.resnet import resnet18,ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_eval_model(model_name):\n",
    "    model = SOResNet(SOBasicBlock, 18).to(device)\n",
    "    path = select_model(model_name)\n",
    "    if path=='ResNet-18':\n",
    "        model = resnet18(ResNet18_Weights).to(device)\n",
    "        model.eval()\n",
    "    elif path=='Untrained':\n",
    "        model = resnet18(weights=None).to(device)\n",
    "        model.eval()\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        model.eval().to(device)\n",
    "    return model\n",
    "\n",
    "CB_SOM_RN_18 = load_and_eval_model('CB_SOM_RN_18').to(device)\n",
    "AB_SOM_RN_18 = load_and_eval_model('AB_SOM_RN_18').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from spacetorch.datasets.animacy_size import Animacy_Size,ANIMACY_SIZE_TRANSFORM\n",
    "from spacetorch.paths import ANIMACY_SIZE_DIR\n",
    "floc_loader = DataLoader(\n",
    "        DatasetRegistry.get(\"fLoc\"), batch_size=256, shuffle=True, num_workers=32, pin_memory=True\n",
    "    )\n",
    "\n",
    "animacy_size_loader = DataLoader(\n",
    "        DatasetRegistry.get(\"animacy-size\"), batch_size=30, shuffle=True, num_workers=8, pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from spacetorch.datasets.floc import CATEGORIES\n",
    "def floc_object(floc_loader):\n",
    "    images_by_category = defaultdict(list)\n",
    "\n",
    "    for idx in range(len(floc_loader.dataset)):\n",
    "        image, target = floc_loader.dataset[idx]\n",
    "        category = CATEGORIES[target]\n",
    "        images_by_category[category].append(image)\n",
    "    images_by_category = {category: torch.stack(images) for category, images in images_by_category.items()}\n",
    "    return images_by_category\n",
    "\n",
    "\n",
    "def floc_catagories(floc_loader):\n",
    "    category_images = {}\n",
    "    floc_object_category = floc_object(floc_loader)\n",
    "    category_images[\"adult_child\"] = torch.cat((floc_object_category[\"adult\"], floc_object_category[\"child\"]), dim=0)\n",
    "    category_images[\"limb_body\"] = torch.cat((floc_object_category[\"limb\"], floc_object_category[\"body\"]), dim=0)\n",
    "    category_images[\"car_instrument\"] = torch.cat((floc_object_category[\"car\"], floc_object_category[\"instrument\"]), dim=0)\n",
    "    category_images[\"corridor_house\"] = torch.cat((floc_object_category[\"corridor\"], floc_object_category[\"house\"]), dim=0)\n",
    "    return category_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacetorch.datasets.animacy_size import TYPES\n",
    "\n",
    "def animacy_size_object(animacy_size_loader):\n",
    "    object_animacy_size = defaultdict(list)\n",
    "\n",
    "    for idx in range(len(animacy_size_loader.dataset)):\n",
    "        image, target = animacy_size_loader.dataset[idx]\n",
    "        category = TYPES[target]\n",
    "        object_animacy_size[category].append(image)\n",
    "    object_animacy_size = {category: torch.stack(images) for category, images in object_animacy_size.items()}\n",
    "    return object_animacy_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_animacy_size(animacy_size_loader):\n",
    "    category_animacy_size = {}\n",
    "    object_animacy_size = animacy_size_object(animacy_size_loader)\n",
    "    category_animacy_size[\"Big\"] = torch.cat((object_animacy_size[\"Big-Animate\"], object_animacy_size[\"Big-Inanimate\"]), dim=0)\n",
    "    category_animacy_size[\"Small\"] = torch.cat((object_animacy_size[\"Small-Animate\"], object_animacy_size[\"Small-Inanimate\"]), dim=0)\n",
    "    category_animacy_size[\"Animate\"] = torch.cat((object_animacy_size[\"Big-Animate\"], object_animacy_size[\"Small-Animate\"]), dim=0)\n",
    "    category_animacy_size[\"Inanimate\"] = torch.cat((object_animacy_size[\"Big-Inanimate\"], object_animacy_size[\"Small-Inanimate\"]), dim=0)\n",
    "    return category_animacy_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_conture_images():\n",
    "    conture_segments = {}\n",
    "    conture,_ = load_v4_images()\n",
    "    bar_images = conture[:16]\n",
    "    corner_images = conture[16:64]\n",
    "    curve_images = conture[64:100]\n",
    "    conture_segments[\"bar\"] = bar_images\n",
    "    conture_segments[\"corner\"] = corner_images\n",
    "    conture_segments[\"curve\"] = curve_images\n",
    "    return conture_segments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name, activation):        \n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "activation = {}\n",
    "\n",
    "def save_hook(net):\n",
    "    for name, module in net.named_modules():\n",
    "        module.register_forward_hook(get_activation(name, activation))\n",
    "    \n",
    "\n",
    "def get_closest_factors(num): \n",
    "    num_root = int(math.sqrt(num))\n",
    "    while num % num_root != 0: \n",
    "        num_root -= 1\n",
    "    return num_root, int(num / num_root)\n",
    "\n",
    "def get_layers(net):\n",
    "    layers = []\n",
    "    for name, layer in net.named_modules():\n",
    "        if isinstance(layer,(SOCONV,SOBasicBlock)):\n",
    "            \n",
    "            layers.append(name)\n",
    "    return layers\n",
    "\n",
    "def get_loc(net):\n",
    "    loc = {}\n",
    "    for name, layer in net.named_modules():\n",
    "        if isinstance(layer,SOCONV):\n",
    "            loc[name] = np.array(layer.neuron_locations())\n",
    "\n",
    "def get_level(target):\n",
    "    if target in ['adult_child', 'limb_body', 'car_instrument', 'corridor_house']:\n",
    "        level = \"category\"\n",
    "    elif target in ['adult', 'body', 'car', 'child', 'corridor', 'house', 'instrument', 'limb', 'number', 'scrambled', 'word']:\n",
    "        level = \"object\"\n",
    "    elif target == \"Big-Small\":\n",
    "        level = \"Size\"\n",
    "    elif target == \"Animate_Inanimate\":\n",
    "        level = \"Animate\"\n",
    "    elif target in [\"corner\",\"curve\",\"bar\"]:\n",
    "        level = \"SI\"\n",
    "    elif target == \"curve_corner\":\n",
    "        level = \"CVCNI\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid category\")\n",
    "    return level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(target):\n",
    "    level = get_level(target)\n",
    "    num_workers = 16\n",
    "    if level == \"category\":\n",
    "        floc_category = floc_catagories(floc_loader)\n",
    "        target_dataloader = DataLoader(floc_category[target], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        other_images_tensor = torch.cat([images for category, images in floc_category.items() if category != target], dim=0)\n",
    "        other_dataloader = DataLoader(other_images_tensor, batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)    \n",
    "    \n",
    "    elif level == \"object\":\n",
    "        floc_object_category = floc_object(floc_loader)\n",
    "        target_dataloader = DataLoader(floc_object_category[target], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        other_images_tensor = torch.cat([images for category, images in floc_object_category.items() if category != target], dim=0)\n",
    "        other_dataloader = DataLoader(other_images_tensor, batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)   \n",
    "    \n",
    "    elif level == \"Size\":\n",
    "        animacy_size = category_animacy_size(animacy_size_loader)\n",
    "        target_dataloader = DataLoader(animacy_size[\"Big\"], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        other_dataloader = DataLoader(animacy_size[\"Small\"], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)   \n",
    "   \n",
    "    elif level == \"Animate\":\n",
    "        animacy_size = category_animacy_size(animacy_size_loader)\n",
    "\n",
    "        target_dataloader = DataLoader(animacy_size[\"Animate\"], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        other_dataloader = DataLoader(animacy_size[\"Inanimate\"], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    elif level == \"SI\":\n",
    "        conture_segments = categorize_conture_images()\n",
    "        target_dataloader = DataLoader(conture_segments[target], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        other_images_tensor = torch.cat([images for category, images in conture_segments.items() if category != target], dim=0)\n",
    "        other_dataloader = DataLoader(other_images_tensor, batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)   \n",
    "   \n",
    "    elif level == \"CVCNI\":\n",
    "        conture_segments = categorize_conture_images()\n",
    "        target_dataloader = DataLoader(conture_segments['curve'], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        other_dataloader = DataLoader(conture_segments['corner'], batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True)  \n",
    "        \n",
    "    return target_dataloader,other_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_list(net, data_loader,target_layers=None):\n",
    "    if target_layers == None:\n",
    "        layers = get_layers(net)\n",
    "    else:\n",
    "        layers = target_layers\n",
    "    all_activation = {}\n",
    "    \n",
    "    with tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Processing Activations\") as pbar:\n",
    "        for i, data in pbar:\n",
    "            _ = net(data.to(device))\n",
    "            if len(all_activation) == 0: \n",
    "                for k in activation.keys():\n",
    "                    if k in layers:\n",
    "                        all_activation[k] = activation[k].cpu().numpy()\n",
    "            else: \n",
    "                for k in activation.keys():\n",
    "                    if k in layers:\n",
    "                        all_activation[k] = np.concatenate([all_activation[k], activation[k].cpu().numpy()])\n",
    "\n",
    "            pbar.set_postfix({\"Processed\": f\"{i+1}/{len(data_loader)}\"})\n",
    "\n",
    "    return all_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVCNI_selectivity(net, target_dataloader, other_dataloader, layer):\n",
    "\n",
    "    target_act = get_activation_list(net, target_dataloader)\n",
    "\n",
    "    other_act = get_activation_list(net, other_dataloader)\n",
    "    \n",
    "    kernel_x, kernel_y = get_closest_factors(target_act[layer].shape[1])\n",
    "    \n",
    "    max_resp_target,_ = torch.from_numpy(target_act[layer]).max(0)\n",
    "\n",
    "    max_resp_target =  max_resp_target.reshape(kernel_x, kernel_y, -1).mean(2)\n",
    "\n",
    "    max_resp_other,_ = torch.from_numpy(other_act[layer]).max(0)\n",
    "\n",
    "    max_resp_other = max_resp_other.reshape(kernel_x, kernel_y, -1).mean(2)\n",
    "        \n",
    "\n",
    "    cvcni = ((max_resp_target - max_resp_other) /(max_resp_target + max_resp_other))\n",
    "    \n",
    "        \n",
    "    return cvcni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def index_selectivity(net, target_dataloader, other_dataloader, layer, thre=0.4, mode='dprime'):\n",
    "    target_act = get_activation_list(net, target_dataloader)\n",
    "    other_act = get_activation_list(net, other_dataloader)\n",
    "    kernel_x, kernel_y = get_closest_factors(target_act[layer].shape[1])\n",
    "\n",
    "    if mode == \"dprime\" or mode == \"discrete\":\n",
    "        target_mean = torch.from_numpy(target_act[layer]).mean(0).reshape(kernel_x, kernel_y, -1)\n",
    "        no_target_mean = torch.from_numpy(other_act[layer]).mean(0).reshape(kernel_x, kernel_y, -1)\n",
    "        target_variance = torch.from_numpy(target_act[layer]).var(0).reshape(kernel_x, kernel_y, -1)\n",
    "        no_target_variance = torch.from_numpy(other_act[layer]).var(0).reshape(kernel_x, kernel_y, -1)\n",
    "\n",
    "        dprime = ((target_mean - no_target_mean) / torch.sqrt((target_variance + no_target_variance) / 2.0)).mean(2)\n",
    "        if mode == \"dprime\":\n",
    "            selectivity = dprime\n",
    "        elif mode == \"discrete\":\n",
    "            selectivity = np.where(dprime >= thre, 1, 0)\n",
    "    elif mode == \"t-test\":\n",
    "        statistic, p_values = ttest_ind(target_act[layer], other_act[layer], equal_var=False,nan_policy='raise')\n",
    "        selectivity = statistic.reshape(kernel_x, kernel_y, -1).mean(-1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode, select: 't-test' , 'dprime' , discrete \")\n",
    "        \n",
    "    return selectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_category(target_level):\n",
    "    if target_level == 'adult_child':\n",
    "        category = 'Face'\n",
    "    elif  target_level =='limb_body':\n",
    "        category = 'Body'\n",
    "    \n",
    "    elif target_level == 'car_instrument':\n",
    "        category = 'Object'\n",
    "\n",
    "    elif target_level == 'corridor_house':\n",
    "        category = 'Place'\n",
    "    elif target_level == 'Animate_Inanimate':\n",
    "        category = 'Animacy'\n",
    "    elif target_level == 'Big-Small':\n",
    "        category = 'Size'\n",
    "\n",
    "    elif target_level == 'angles':\n",
    "        category = 'Preferred Orientation'\n",
    "    elif target_level == 'sfs':\n",
    "        category = 'Spatial Frequency'\n",
    "    else:\n",
    "        category = target_level\n",
    "\n",
    "    return category\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_selectivity_idx(net,layer,target_category,**kwargs):\n",
    "    mode = kwargs.get('mode') \n",
    "    thre = kwargs.get('thre')\n",
    "    print(mode)\n",
    "    save_hook(net)\n",
    "    target_dataloader, other_dataloader = make_dataloader(target_category)\n",
    "    dprime = index_selectivity(net,target_dataloader,other_dataloader,layer,mode=mode,thre=thre)\n",
    "    return dprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as nd\n",
    "\n",
    "def plot_dprime(model_type, layer, target_category, cmap='plasma', save=False, format='png', smoothed=True, **kwargs):\n",
    "    net = load_and_eval_model(model_type).to(device)\n",
    "    selectivity = category_selectivity_idx(net, layer, target_category, **kwargs)\n",
    "    if smoothed:\n",
    "        selectivity = nd.gaussian_filter(selectivity, sigma=0.5)\n",
    "    category = map_to_category(target_level=target_category)\n",
    "\n",
    "    fig, ax = plt.subplots()  # Create a figure and a set of subplots\n",
    "\n",
    "    vmin_rounded = math.floor((selectivity.min()) * 10) / 10\n",
    "    vmax_rounded = math.ceil((selectivity.max()) * 10) / 10\n",
    "    cax = ax.matshow(selectivity, cmap=cmap,vmin= vmin_rounded,vmax = vmax_rounded )  # Use the ax object for matshow\n",
    "\n",
    "\n",
    "    if save:\n",
    "        save_path = f\"{FIGURE_DIR}/Manuscripts/Category_selectivity/{model_type}/{category}\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        print(save_path)\n",
    "\n",
    "        # Adjust colorbar size to match ax\n",
    "        cbar = fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "        ax.axis('off')\n",
    "        plt.savefig(f'{save_path}_{layer}.{format}', format=format)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "    else:\n",
    "        # Adjust colorbar size to match ax\n",
    "        cbar = fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        ax.set_title(f'{category}_{layer}')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dprime('ResNet-18','layer4.1.conv1','adult_child')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacetorch.utils.array_utils import lower_tri, midpoints_from_bin_edges,flatten\n",
    "\n",
    "def delta_selectivity(dprime,metric=None):\n",
    "    dprime = dprime.reshape(-1)\n",
    "    \n",
    "    pairwise_differences =  []\n",
    "    for x in range (dprime.shape[0]):\n",
    "        for y in range (x+1,dprime.shape[0]):\n",
    "            difference = abs(dprime[x] - dprime[y].item())\n",
    "            pairwise_differences.append(difference)\n",
    "\n",
    "    pairwise_differences_torch = torch.tensor(pairwise_differences)\n",
    "\n",
    "    if metric == 'angles':\n",
    "        pairwise_differences_torch[np.where(pairwise_differences_torch >= 90)] = 180 - pairwise_differences_torch[np.where(pairwise_differences_torch >= 90)]\n",
    "    \n",
    "    return pairwise_differences_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from utilities import neuron_locations\n",
    
    "\n",
    "def normalized_random_shuffeling(dim1,dim2,pairwise_differences_torch,normalize=True):\n",
    "    distances = torch.Tensor(list(neuron_locations(F1=dim1,F2=dim2)))\n",
    "    distances_kernels = cdist(distances, distances, metric='euclidean')\n",
    "    distances_kernels = distances_kernels[np.triu_indices(len(distances), k=1)]\n",
    "    unique_distances = np.unique(distances_kernels)\n",
    "    selectivity_per_distance = [pairwise_differences_torch[distances_kernels == dist].mean() for dist in unique_distances]\n",
    "    if normalize:\n",
    "        copy_difference_per_distance = pairwise_differences_torch\n",
    "        shuffled_tensors = torch.empty((copy_difference_per_distance.size()[0], 0))\n",
    "        \n",
    "\n",
    "        for _ in range(1000):\n",
    "\n",
    "            shuffled_tensor = copy_difference_per_distance[torch.randperm(copy_difference_per_distance.size()[0])]\n",
    "            shuffled_tensor = shuffled_tensor.unsqueeze(1)  \n",
    "            shuffled_tensors=torch.cat((shuffled_tensors,shuffled_tensor),dim=1)\n",
    "\n",
    "\n",
    "        shuffled_tensor = torch.mean(shuffled_tensor,1)\n",
    "\n",
    "        shuffled_tensor = [shuffled_tensor[distances_kernels == dist].mean() for dist in unique_distances]\n",
    "\n",
    "        normalized_tensor = [corr / shuffled for corr, shuffled in zip(selectivity_per_distance, shuffled_tensor)]\n",
    "    else:\n",
    "        normalized_tensor = selectivity_per_distance\n",
    "\n",
    "    return normalized_tensor,unique_distances\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def pairwise_selectivity_distance(model_name,layer_name='layer4.1.soconv1',target_category='adult_child',step=4):\n",
    "\n",
    "    model = load_and_eval_model(model_name)\n",
    "\n",
    "    dprime = category_selectivity_idx(model,layer=layer_name,target_category=target_category,mode='dprime')\n",
    "    \n",
    "    pairwise_differences_torch = delta_selectivity(dprime)\n",
    "    \n",
    "\n",
    "    normalized_tensor,unique_distances = normalized_random_shuffeling(dim1=dprime.shape[0],dim2=dprime.shape[1],pairwise_differences_torch=pairwise_differences_torch)\n",
    "\n",
    "    selected_distances = unique_distances[::step]\n",
    "    selected_normalized_tensor = normalized_tensor[::step]\n",
    "    font = FontProperties(family='New Times Roman',size=\"large\",weight='bold')\n",
    "\n",
    "    plt.plot(selected_distances, selected_normalized_tensor, marker='o', linestyle='-')\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.title(f'model: {model_name}_{target_category}_selectivity')\n",
    "    plt.xlabel('Pair-wise Euclidean Distance', fontsize=12,fontproperties=font)\n",
    "    plt.ylabel(r\"$\\Delta$ Selectivity\" \"\\n\" \"(vs. Chance)\", fontsize=12,fontproperties=font)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(0,2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_selectivity_distance('CB_SOM_RN_18',layer_name='layer4.0.soconv1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDANN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
